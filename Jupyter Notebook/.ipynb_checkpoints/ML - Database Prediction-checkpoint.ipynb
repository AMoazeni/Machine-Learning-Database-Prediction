{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<br></br>\n",
    "Take me to the [code and Jupyter Notebook](https://github.com/AMoazeni/Machine-Learning-Database-Prediction/blob/master/Jupyter%20Notebook/ML%20-%20Database%20Prediction.ipynb) for Database Prediction!\n",
    "\n",
    "<br></br>\n",
    "This article shows you the theory and code behind a popular Machine Learning technique called Artificial Neural Network (ANN) which is a common Deep Learning algorithm.\n",
    "\n",
    "<br></br>\n",
    "You are provided with a Bank database with 10,000 customers. You're training an ML model to predict the likelihood of a customer leaving or staying with the bank. This code is robust in architecture and can be modified to predict all sorts of customer behavior, provided with enough data.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Artificial Neural Networks\n",
    "\n",
    "<br></br>\n",
    "Let's take a look at the theory behind the Artificial Neural Network algorithm which was popularized by Geoffrey Hinton in the 1980's and is used in Deep Machine Learning. \"Deep\" in Deep Learning refers to all the hidden layers used in this type of Dynamic Programming algorithm.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The input layer observations and related output refer to ONE row of data. Adjustment of weights is how Neural Nets learn, they decide the strength and importance of signals that are passed along or blocked by an Activation Function. They keep adjusting weights until the predicted output closely matches the actual output.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/01%20-%20Deep%20Learning.png\" alt=\"Deep-Learning\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Here is a zoomed in version of the node diagram. Yellow nodes represent inputs, green nodes are the hidden layers, and red nodes are outputs.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/02%20-%20Neuron.png\" alt=\"Neuron\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "<br></br>\n",
    "Feature Scaling (Standardize or Normalize) is applied to input variables. This makes it easy for Neural Nets to process data by bringing input values close to each other, read 'Efficient Back Propagation.pdf' in the research papers section.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/02_1%20-%20Standardized%20Equation.png\" alt=\"Standardize\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/02_2%20-%20Normalized%20Equation.png\" alt=\"Normalize\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Activation Function\n",
    "\n",
    "<br></br>\n",
    "Here is a list of some Neural Network Activation Functions. Read 'Deep sparse rectifier neural networks.pdf' in the research papers section.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "1. Threshold Function - Rigid binary style function\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/03%20-%20Threshold.png\" width=\"400\" alt=\"Threshold\"></div>\n",
    "\n",
    "2. Sigmoid Function - Smooth, good for output Layers that predict probability\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/04%20-%20Sigmoid.png\" width=\"400\" alt=\"Sigmoid\"></div>\n",
    "\n",
    "3. Rectifier Function - Gradually increases as input Value increases\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/05%20-%20Rectifier.png\" width=\"400\" alt=\"Rectifier\"></div>\n",
    "\n",
    "4. Hyperbolic Tangent Function - Similar to Sigmoid Function but values can go below zero\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/06%20-%20Tanh.png\" width=\"400\" alt=\"Tanh\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Different layers of a Neural Net can use different Activation Functions.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/07%20-%20NN%20Activation%20Example.png\" width=\"600\" alt=\"Activation\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Cost Function\n",
    "\n",
    "<br></br>\n",
    "The Cost Function is a plot of the differences between the target and the network's output, which we try to minimize through weight adjustments (Backpropagation) in epochs (one training cycle on the Training Set). Once input information is fed through the network and a y_hat output estimate is found (Forward-propagation), we take the error and go back through the network and adjust the weights (Backpropagation Algorithm). The most common cost function is the Quadratic (Root Mean Square) cost:\n",
    "\n",
    "\n",
    "<br></br>\n",
    "$$\n",
    "Cost = \\frac{(\\hat y - y)^2}{2} = \\frac{(Wighted Estimate - Actual)^2}{2} \n",
    "$$\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Read this [Deep Learning Book](http://neuralnetworksanddeeplearning.com/index.html) and this [List of Cost Functions Uses](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications?).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Batch Gradient Descent\n",
    "\n",
    "<br></br>\n",
    "This is a Cost minimization technique that looks for downhill slopes and works on Convex Cost Functions. The function can have any number of dimensions, but we are only able to visualize up to three dimensions.\n",
    "\n",
    "\n",
    "### 1D Gradient Descent\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/09%20-%20Gradient%20Descent%201D.png\" width=\"600\" alt=\"Gradient-Descent-1D\"></div>\n",
    "\n",
    "\n",
    "### 2D Gradient Descent\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/10%20-%20Gradient%20Descent%202D.png\" width=\"300\"  alt=\"Gradient-Descent-2D\"></div>\n",
    "\n",
    "\n",
    "### 3D Gradient Descent\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/11%20-%20Gradient%20Descent%203D.png\" width=\"600\" alt=\"Gradient-Descent-3D\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Reinforcement Learning (Stochastic Gradient Descent)\n",
    "\n",
    "<br></br>\n",
    "This method is faster & more accurate than Batch Gradient Descent.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "In order to avoid the Local Minimum trap, we can take more sporadic steps in random directions to increase the likelihood of finding the Global Minimum. We can achieve this by adjusting weights one row at a time (Stochastic Gradient Descent) instead of all-at-once (Batch Gradient Descent). Read 'Neural Network in 13 lines of Python.pdf' in the research papers section.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/12%20-%20Local%20Min%20Trap.png\" alt=\"Local-Minimum\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "These are the steps for Stochastic Gradient Descent:\n",
    "1. Initialize weights to small numbers close to 0 (but NOT 0)\n",
    "2. Input first row of Observation Data into input layer\n",
    "3. Forward-propagate: Apply weights to inputs to get predicted result 'y_hat'\n",
    "4. Compute Error = 'y_hat' - 'y_actual'\n",
    "5. Back-propagate: Update weights according to the Learning Rate and how much they're responsible for the Error.\n",
    "6. Repeat steps 1-5 after each observation (Reinforcement Learning), or after each batch (Batch Gradient Descent)\n",
    "7. Epoch is the Training Set passing through the Artificial Neural Network, more Epochs yield improved results.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Evaluating the Artificial Neural Network\n",
    "\n",
    "<br></br>\n",
    "Be careful when measuring the accuracy of a model. Bias and Variance can differ every time the model is evaluated. To solve this problem, we can use K-Fold Cross Validation which splits the data into multiple segments and averages overall accuracy.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/13%20-%20Bias-Variance%20Tradeoff.png\" width=\"400\" alt=\"Bias-Variance\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Jupyter%20Notebook/Images/14%20-%20K-Fold%20Cross%20Validation.png\" width=\"400\" alt=\"Cross-Validation\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Overfitting\n",
    "\n",
    "<br></br>\n",
    "Overfitting is when your model is over-trained on the Training Set and isn't generalized enough. This reduces performance on Test Set predictions.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Indicators of overfitting:\n",
    "\n",
    "1. Training and Test Accuracies have a large difference\n",
    "2. Observing High Accuracy Variance when applying K-Fold Cross Validation\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Solve overfitting with \"Dropout Regularization\", this randomly disables Neurons through iterations so they don't grow too dependent on each other. This helps the Neural Network learns several independent correlations from the data.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Sample Problem - Bank Database Prediction\n",
    "\n",
    "<br></br>\n",
    "Let's test our knowledge of Artificial Neural Networks by solving a real world problem. Take a look at 'Bank_Customer_Data.csv' in the Data folder of this repository. This technique can be applied to any or any customer oriented business data set.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "### Problem Description:\n",
    "\n",
    "A Bank (or any business) is trying to improve customer retention. The Bank engineers have put together a table of data about their customers (Name, Age, Location, Income, etc). They also have data on whether customers left the Bank or stayed with them (last column of data).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The Bank is trying to build a Machine Learning model that predicts the likelihood of a customer leaving before it actually happens so they can work on improving customer satisfaction.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "### Code\n",
    "\n",
    "<br></br>\n",
    "You can run the code online with [Google Colab](https://colab.research.google.com/drive/1fkkPPombnFH7_A8dlOkia2P0SZWMVt7o) which is a web based Jupyter Notebook environment and doesn't require installations. \n",
    "\n",
    "\n",
    "<br></br>\n",
    "The better alternative is to download the code and run it with 'Jupyter Notebook' or copy the code into the 'Spyder' IDE found in the [Anaconda Distribution](https://www.anaconda.com/download/). 'Spyder' is similar to MATLAB, it allows you to step through the code and examine the 'Variable Explorer' to see exactly how the data is parsed and analyzed. Jupyter Notebook also offers a [Jupyter Variable Explorer Extension](http://volderette.de/jupyter-notebook-variable-explorer/) which is quite useful for keeping track of variables.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "```shell\n",
    "$ git clone https://github.com/AMoazeni/Machine-Learning-Database-Prediction.git\n",
    "$ cd Machine-Learning-Database-Prediction\n",
    "```\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>15574012</td>\n",
       "      <td>Chu</td>\n",
       "      <td>645</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>15592531</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>822</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>15656148</td>\n",
       "      <td>Obinna</td>\n",
       "      <td>376</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>15792365</td>\n",
       "      <td>He</td>\n",
       "      <td>501</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>15592389</td>\n",
       "      <td>H?</td>\n",
       "      <td>684</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>15767821</td>\n",
       "      <td>Bearce</td>\n",
       "      <td>528</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>102016.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80181.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>15737173</td>\n",
       "      <td>Andrews</td>\n",
       "      <td>497</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76390.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>15632264</td>\n",
       "      <td>Kay</td>\n",
       "      <td>476</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26260.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>15691483</td>\n",
       "      <td>Chin</td>\n",
       "      <td>549</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190857.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>15600882</td>\n",
       "      <td>Scott</td>\n",
       "      <td>635</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65951.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>15643966</td>\n",
       "      <td>Goforth</td>\n",
       "      <td>616</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>143129.41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64327.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>15737452</td>\n",
       "      <td>Romeo</td>\n",
       "      <td>653</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>132602.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5097.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>15788218</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>549</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14406.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>15661507</td>\n",
       "      <td>Muldrow</td>\n",
       "      <td>587</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158684.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>15568982</td>\n",
       "      <td>Hao</td>\n",
       "      <td>726</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54724.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>15577657</td>\n",
       "      <td>McDonald</td>\n",
       "      <td>732</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170886.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>15597945</td>\n",
       "      <td>Dellucci</td>\n",
       "      <td>636</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138555.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>15699309</td>\n",
       "      <td>Gerasimov</td>\n",
       "      <td>510</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118913.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>15725737</td>\n",
       "      <td>Mosman</td>\n",
       "      <td>669</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8487.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>15625047</td>\n",
       "      <td>Yen</td>\n",
       "      <td>846</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>187616.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>15738191</td>\n",
       "      <td>Maclean</td>\n",
       "      <td>577</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>124508.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>15736816</td>\n",
       "      <td>Young</td>\n",
       "      <td>756</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>136815.64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170041.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>15700772</td>\n",
       "      <td>Nebechi</td>\n",
       "      <td>571</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38433.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>15728693</td>\n",
       "      <td>McWilliams</td>\n",
       "      <td>574</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>141349.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100187.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>15656300</td>\n",
       "      <td>Lucciano</td>\n",
       "      <td>411</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>59697.17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53483.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>9971</td>\n",
       "      <td>15587133</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>518</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>151027.05</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119377.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>9972</td>\n",
       "      <td>15721377</td>\n",
       "      <td>Chou</td>\n",
       "      <td>833</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>144751.81</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166472.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>9973</td>\n",
       "      <td>15747927</td>\n",
       "      <td>Ch'in</td>\n",
       "      <td>758</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>155739.76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>171552.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>9974</td>\n",
       "      <td>15806455</td>\n",
       "      <td>Miller</td>\n",
       "      <td>611</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>157474.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>9975</td>\n",
       "      <td>15695474</td>\n",
       "      <td>Barker</td>\n",
       "      <td>583</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>122531.86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13549.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>9976</td>\n",
       "      <td>15666295</td>\n",
       "      <td>Smith</td>\n",
       "      <td>610</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>113957.01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>196526.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>9977</td>\n",
       "      <td>15656062</td>\n",
       "      <td>Azikiwe</td>\n",
       "      <td>637</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>103377.81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84419.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>9978</td>\n",
       "      <td>15579969</td>\n",
       "      <td>Mancini</td>\n",
       "      <td>683</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24991.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>9979</td>\n",
       "      <td>15703563</td>\n",
       "      <td>P'eng</td>\n",
       "      <td>774</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>93017.47</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>191608.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>9980</td>\n",
       "      <td>15692664</td>\n",
       "      <td>Diribe</td>\n",
       "      <td>677</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>90022.85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2988.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>9981</td>\n",
       "      <td>15719276</td>\n",
       "      <td>T'ao</td>\n",
       "      <td>741</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>74371.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99595.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>9982</td>\n",
       "      <td>15672754</td>\n",
       "      <td>Burbidge</td>\n",
       "      <td>498</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>152039.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53445.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>9983</td>\n",
       "      <td>15768163</td>\n",
       "      <td>Griffin</td>\n",
       "      <td>655</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>137145.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>115146.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>9984</td>\n",
       "      <td>15656710</td>\n",
       "      <td>Cocci</td>\n",
       "      <td>613</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151325.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>9985</td>\n",
       "      <td>15696175</td>\n",
       "      <td>Echezonachukwu</td>\n",
       "      <td>602</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>90602.42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51695.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>9986</td>\n",
       "      <td>15586914</td>\n",
       "      <td>Nepean</td>\n",
       "      <td>659</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>123841.49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96833.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>9987</td>\n",
       "      <td>15581736</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>673</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>183579.54</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34047.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>9988</td>\n",
       "      <td>15588839</td>\n",
       "      <td>Mancini</td>\n",
       "      <td>606</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>180307.73</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1914.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>9989</td>\n",
       "      <td>15589329</td>\n",
       "      <td>Pirozzi</td>\n",
       "      <td>775</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49337.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>9990</td>\n",
       "      <td>15605622</td>\n",
       "      <td>McMillan</td>\n",
       "      <td>841</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>179436.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>9991</td>\n",
       "      <td>15798964</td>\n",
       "      <td>Nkemakonam</td>\n",
       "      <td>714</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>35016.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53667.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>9992</td>\n",
       "      <td>15769959</td>\n",
       "      <td>Ajuluchukwu</td>\n",
       "      <td>597</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>88381.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69384.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>9993</td>\n",
       "      <td>15657105</td>\n",
       "      <td>Chukwualuka</td>\n",
       "      <td>726</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>195192.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>9994</td>\n",
       "      <td>15569266</td>\n",
       "      <td>Rahman</td>\n",
       "      <td>644</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>155060.41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29179.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>9995</td>\n",
       "      <td>15719294</td>\n",
       "      <td>Wood</td>\n",
       "      <td>800</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167773.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId         Surname  CreditScore Geography  Gender  \\\n",
       "0             1    15634602        Hargrave          619    France  Female   \n",
       "1             2    15647311            Hill          608     Spain  Female   \n",
       "2             3    15619304            Onio          502    France  Female   \n",
       "3             4    15701354            Boni          699    France  Female   \n",
       "4             5    15737888        Mitchell          850     Spain  Female   \n",
       "5             6    15574012             Chu          645     Spain    Male   \n",
       "6             7    15592531        Bartlett          822    France    Male   \n",
       "7             8    15656148          Obinna          376   Germany  Female   \n",
       "8             9    15792365              He          501    France    Male   \n",
       "9            10    15592389              H?          684    France    Male   \n",
       "10           11    15767821          Bearce          528    France    Male   \n",
       "11           12    15737173         Andrews          497     Spain    Male   \n",
       "12           13    15632264             Kay          476    France  Female   \n",
       "13           14    15691483            Chin          549    France  Female   \n",
       "14           15    15600882           Scott          635     Spain  Female   \n",
       "15           16    15643966         Goforth          616   Germany    Male   \n",
       "16           17    15737452           Romeo          653   Germany    Male   \n",
       "17           18    15788218       Henderson          549     Spain  Female   \n",
       "18           19    15661507         Muldrow          587     Spain    Male   \n",
       "19           20    15568982             Hao          726    France  Female   \n",
       "20           21    15577657        McDonald          732    France    Male   \n",
       "21           22    15597945        Dellucci          636     Spain  Female   \n",
       "22           23    15699309       Gerasimov          510     Spain  Female   \n",
       "23           24    15725737          Mosman          669    France    Male   \n",
       "24           25    15625047             Yen          846    France  Female   \n",
       "25           26    15738191         Maclean          577    France    Male   \n",
       "26           27    15736816           Young          756   Germany    Male   \n",
       "27           28    15700772         Nebechi          571    France    Male   \n",
       "28           29    15728693      McWilliams          574   Germany  Female   \n",
       "29           30    15656300        Lucciano          411    France    Male   \n",
       "...         ...         ...             ...          ...       ...     ...   \n",
       "9970       9971    15587133        Thompson          518    France    Male   \n",
       "9971       9972    15721377            Chou          833    France  Female   \n",
       "9972       9973    15747927           Ch'in          758    France    Male   \n",
       "9973       9974    15806455          Miller          611    France    Male   \n",
       "9974       9975    15695474          Barker          583    France    Male   \n",
       "9975       9976    15666295           Smith          610   Germany    Male   \n",
       "9976       9977    15656062         Azikiwe          637    France  Female   \n",
       "9977       9978    15579969         Mancini          683    France  Female   \n",
       "9978       9979    15703563           P'eng          774    France    Male   \n",
       "9979       9980    15692664          Diribe          677    France  Female   \n",
       "9980       9981    15719276            T'ao          741     Spain    Male   \n",
       "9981       9982    15672754        Burbidge          498   Germany    Male   \n",
       "9982       9983    15768163         Griffin          655   Germany  Female   \n",
       "9983       9984    15656710           Cocci          613    France    Male   \n",
       "9984       9985    15696175  Echezonachukwu          602   Germany    Male   \n",
       "9985       9986    15586914          Nepean          659    France    Male   \n",
       "9986       9987    15581736        Bartlett          673   Germany    Male   \n",
       "9987       9988    15588839         Mancini          606     Spain    Male   \n",
       "9988       9989    15589329         Pirozzi          775    France    Male   \n",
       "9989       9990    15605622        McMillan          841     Spain    Male   \n",
       "9990       9991    15798964      Nkemakonam          714   Germany    Male   \n",
       "9991       9992    15769959     Ajuluchukwu          597    France  Female   \n",
       "9992       9993    15657105     Chukwualuka          726     Spain    Male   \n",
       "9993       9994    15569266          Rahman          644    France    Male   \n",
       "9994       9995    15719294            Wood          800    France  Female   \n",
       "9995       9996    15606229        Obijiaku          771    France    Male   \n",
       "9996       9997    15569892       Johnstone          516    France    Male   \n",
       "9997       9998    15584532             Liu          709    France  Female   \n",
       "9998       9999    15682355       Sabbatini          772   Germany    Male   \n",
       "9999      10000    15628319          Walker          792    France  Female   \n",
       "\n",
       "      Age  Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0      42       2       0.00              1          1               1   \n",
       "1      41       1   83807.86              1          0               1   \n",
       "2      42       8  159660.80              3          1               0   \n",
       "3      39       1       0.00              2          0               0   \n",
       "4      43       2  125510.82              1          1               1   \n",
       "5      44       8  113755.78              2          1               0   \n",
       "6      50       7       0.00              2          1               1   \n",
       "7      29       4  115046.74              4          1               0   \n",
       "8      44       4  142051.07              2          0               1   \n",
       "9      27       2  134603.88              1          1               1   \n",
       "10     31       6  102016.72              2          0               0   \n",
       "11     24       3       0.00              2          1               0   \n",
       "12     34      10       0.00              2          1               0   \n",
       "13     25       5       0.00              2          0               0   \n",
       "14     35       7       0.00              2          1               1   \n",
       "15     45       3  143129.41              2          0               1   \n",
       "16     58       1  132602.88              1          1               0   \n",
       "17     24       9       0.00              2          1               1   \n",
       "18     45       6       0.00              1          0               0   \n",
       "19     24       6       0.00              2          1               1   \n",
       "20     41       8       0.00              2          1               1   \n",
       "21     32       8       0.00              2          1               0   \n",
       "22     38       4       0.00              1          1               0   \n",
       "23     46       3       0.00              2          0               1   \n",
       "24     38       5       0.00              1          1               1   \n",
       "25     25       3       0.00              2          0               1   \n",
       "26     36       2  136815.64              1          1               1   \n",
       "27     44       9       0.00              2          0               0   \n",
       "28     43       3  141349.43              1          1               1   \n",
       "29     29       0   59697.17              2          1               1   \n",
       "...   ...     ...        ...            ...        ...             ...   \n",
       "9970   42       7  151027.05              2          1               0   \n",
       "9971   34       3  144751.81              1          0               0   \n",
       "9972   26       4  155739.76              1          1               0   \n",
       "9973   27       7       0.00              2          1               1   \n",
       "9974   33       7  122531.86              1          1               0   \n",
       "9975   50       1  113957.01              2          1               0   \n",
       "9976   33       7  103377.81              1          1               0   \n",
       "9977   32       9       0.00              2          1               1   \n",
       "9978   40       9   93017.47              2          1               0   \n",
       "9979   58       1   90022.85              1          0               1   \n",
       "9980   35       6   74371.49              1          0               0   \n",
       "9981   42       3  152039.70              1          1               1   \n",
       "9982   46       7  137145.12              1          1               0   \n",
       "9983   40       4       0.00              1          0               0   \n",
       "9984   35       7   90602.42              2          1               1   \n",
       "9985   36       6  123841.49              2          1               0   \n",
       "9986   47       1  183579.54              2          0               1   \n",
       "9987   30       8  180307.73              2          1               1   \n",
       "9988   30       4       0.00              2          1               0   \n",
       "9989   28       4       0.00              2          1               1   \n",
       "9990   33       3   35016.60              1          1               0   \n",
       "9991   53       4   88381.21              1          1               0   \n",
       "9992   36       2       0.00              1          1               0   \n",
       "9993   28       7  155060.41              1          1               0   \n",
       "9994   29       2       0.00              2          0               0   \n",
       "9995   39       5       0.00              2          1               0   \n",
       "9996   35      10   57369.61              1          1               1   \n",
       "9997   36       7       0.00              1          0               1   \n",
       "9998   42       3   75075.31              2          1               0   \n",
       "9999   28       4  130142.79              1          1               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "0           101348.88       1  \n",
       "1           112542.58       0  \n",
       "2           113931.57       1  \n",
       "3            93826.63       0  \n",
       "4            79084.10       0  \n",
       "5           149756.71       1  \n",
       "6            10062.80       0  \n",
       "7           119346.88       1  \n",
       "8            74940.50       0  \n",
       "9            71725.73       0  \n",
       "10           80181.12       0  \n",
       "11           76390.01       0  \n",
       "12           26260.98       0  \n",
       "13          190857.79       0  \n",
       "14           65951.65       0  \n",
       "15           64327.26       0  \n",
       "16            5097.67       1  \n",
       "17           14406.41       0  \n",
       "18          158684.81       0  \n",
       "19           54724.03       0  \n",
       "20          170886.17       0  \n",
       "21          138555.46       0  \n",
       "22          118913.53       1  \n",
       "23            8487.75       0  \n",
       "24          187616.16       0  \n",
       "25          124508.29       0  \n",
       "26          170041.95       0  \n",
       "27           38433.35       0  \n",
       "28          100187.43       0  \n",
       "29           53483.21       0  \n",
       "...               ...     ...  \n",
       "9970        119377.36       0  \n",
       "9971        166472.81       0  \n",
       "9972        171552.02       0  \n",
       "9973        157474.10       0  \n",
       "9974         13549.24       0  \n",
       "9975        196526.55       1  \n",
       "9976         84419.78       0  \n",
       "9977         24991.92       0  \n",
       "9978        191608.97       0  \n",
       "9979          2988.28       0  \n",
       "9980         99595.67       0  \n",
       "9981         53445.17       1  \n",
       "9982        115146.40       1  \n",
       "9983        151325.24       0  \n",
       "9984         51695.41       0  \n",
       "9985         96833.00       0  \n",
       "9986         34047.54       0  \n",
       "9987          1914.41       0  \n",
       "9988         49337.84       0  \n",
       "9989        179436.60       0  \n",
       "9990         53667.08       0  \n",
       "9991         69384.71       1  \n",
       "9992        195192.40       0  \n",
       "9993         29179.52       0  \n",
       "9994        167773.55       0  \n",
       "9995         96270.64       0  \n",
       "9996        101699.77       0  \n",
       "9997         42085.58       1  \n",
       "9998         92888.52       1  \n",
       "9999         38190.78       0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X input values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>645</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>822</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>376</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>501</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>684</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>528</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>102016.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80181.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>497</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76390.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>476</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26260.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>549</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190857.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>635</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65951.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>616</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>143129.41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64327.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>653</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>132602.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5097.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>549</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14406.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>587</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158684.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>726</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54724.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>732</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170886.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>636</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138555.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>510</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118913.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>669</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8487.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>846</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>187616.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>577</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>124508.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>756</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>136815.64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170041.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>571</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38433.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>574</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>141349.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100187.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>411</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>59697.17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53483.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>518</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>151027.05</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119377.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>833</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>144751.81</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166472.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>758</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>155739.76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>171552.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>611</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>157474.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>583</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>122531.86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13549.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>610</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>113957.01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>196526.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>637</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>103377.81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84419.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>683</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24991.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>774</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>93017.47</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>191608.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>677</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>90022.85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2988.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>741</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>74371.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99595.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>498</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>152039.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53445.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>655</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>137145.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>115146.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>613</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151325.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>602</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>90602.42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51695.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>659</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>123841.49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96833.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>673</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>183579.54</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34047.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>606</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>180307.73</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1914.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>775</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49337.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>841</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>179436.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>714</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>35016.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53667.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>597</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>88381.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69384.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>726</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>195192.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>644</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>155060.41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29179.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>800</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167773.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0             619    France  Female   42       2       0.00              1   \n",
       "1             608     Spain  Female   41       1   83807.86              1   \n",
       "2             502    France  Female   42       8  159660.80              3   \n",
       "3             699    France  Female   39       1       0.00              2   \n",
       "4             850     Spain  Female   43       2  125510.82              1   \n",
       "5             645     Spain    Male   44       8  113755.78              2   \n",
       "6             822    France    Male   50       7       0.00              2   \n",
       "7             376   Germany  Female   29       4  115046.74              4   \n",
       "8             501    France    Male   44       4  142051.07              2   \n",
       "9             684    France    Male   27       2  134603.88              1   \n",
       "10            528    France    Male   31       6  102016.72              2   \n",
       "11            497     Spain    Male   24       3       0.00              2   \n",
       "12            476    France  Female   34      10       0.00              2   \n",
       "13            549    France  Female   25       5       0.00              2   \n",
       "14            635     Spain  Female   35       7       0.00              2   \n",
       "15            616   Germany    Male   45       3  143129.41              2   \n",
       "16            653   Germany    Male   58       1  132602.88              1   \n",
       "17            549     Spain  Female   24       9       0.00              2   \n",
       "18            587     Spain    Male   45       6       0.00              1   \n",
       "19            726    France  Female   24       6       0.00              2   \n",
       "20            732    France    Male   41       8       0.00              2   \n",
       "21            636     Spain  Female   32       8       0.00              2   \n",
       "22            510     Spain  Female   38       4       0.00              1   \n",
       "23            669    France    Male   46       3       0.00              2   \n",
       "24            846    France  Female   38       5       0.00              1   \n",
       "25            577    France    Male   25       3       0.00              2   \n",
       "26            756   Germany    Male   36       2  136815.64              1   \n",
       "27            571    France    Male   44       9       0.00              2   \n",
       "28            574   Germany  Female   43       3  141349.43              1   \n",
       "29            411    France    Male   29       0   59697.17              2   \n",
       "...           ...       ...     ...  ...     ...        ...            ...   \n",
       "9970          518    France    Male   42       7  151027.05              2   \n",
       "9971          833    France  Female   34       3  144751.81              1   \n",
       "9972          758    France    Male   26       4  155739.76              1   \n",
       "9973          611    France    Male   27       7       0.00              2   \n",
       "9974          583    France    Male   33       7  122531.86              1   \n",
       "9975          610   Germany    Male   50       1  113957.01              2   \n",
       "9976          637    France  Female   33       7  103377.81              1   \n",
       "9977          683    France  Female   32       9       0.00              2   \n",
       "9978          774    France    Male   40       9   93017.47              2   \n",
       "9979          677    France  Female   58       1   90022.85              1   \n",
       "9980          741     Spain    Male   35       6   74371.49              1   \n",
       "9981          498   Germany    Male   42       3  152039.70              1   \n",
       "9982          655   Germany  Female   46       7  137145.12              1   \n",
       "9983          613    France    Male   40       4       0.00              1   \n",
       "9984          602   Germany    Male   35       7   90602.42              2   \n",
       "9985          659    France    Male   36       6  123841.49              2   \n",
       "9986          673   Germany    Male   47       1  183579.54              2   \n",
       "9987          606     Spain    Male   30       8  180307.73              2   \n",
       "9988          775    France    Male   30       4       0.00              2   \n",
       "9989          841     Spain    Male   28       4       0.00              2   \n",
       "9990          714   Germany    Male   33       3   35016.60              1   \n",
       "9991          597    France  Female   53       4   88381.21              1   \n",
       "9992          726     Spain    Male   36       2       0.00              1   \n",
       "9993          644    France    Male   28       7  155060.41              1   \n",
       "9994          800    France  Female   29       2       0.00              2   \n",
       "9995          771    France    Male   39       5       0.00              2   \n",
       "9996          516    France    Male   35      10   57369.61              1   \n",
       "9997          709    France  Female   36       7       0.00              1   \n",
       "9998          772   Germany    Male   42       3   75075.31              2   \n",
       "9999          792    France  Female   28       4  130142.79              1   \n",
       "\n",
       "      HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0             1               1        101348.88  \n",
       "1             0               1        112542.58  \n",
       "2             1               0        113931.57  \n",
       "3             0               0         93826.63  \n",
       "4             1               1         79084.10  \n",
       "5             1               0        149756.71  \n",
       "6             1               1         10062.80  \n",
       "7             1               0        119346.88  \n",
       "8             0               1         74940.50  \n",
       "9             1               1         71725.73  \n",
       "10            0               0         80181.12  \n",
       "11            1               0         76390.01  \n",
       "12            1               0         26260.98  \n",
       "13            0               0        190857.79  \n",
       "14            1               1         65951.65  \n",
       "15            0               1         64327.26  \n",
       "16            1               0          5097.67  \n",
       "17            1               1         14406.41  \n",
       "18            0               0        158684.81  \n",
       "19            1               1         54724.03  \n",
       "20            1               1        170886.17  \n",
       "21            1               0        138555.46  \n",
       "22            1               0        118913.53  \n",
       "23            0               1          8487.75  \n",
       "24            1               1        187616.16  \n",
       "25            0               1        124508.29  \n",
       "26            1               1        170041.95  \n",
       "27            0               0         38433.35  \n",
       "28            1               1        100187.43  \n",
       "29            1               1         53483.21  \n",
       "...         ...             ...              ...  \n",
       "9970          1               0        119377.36  \n",
       "9971          0               0        166472.81  \n",
       "9972          1               0        171552.02  \n",
       "9973          1               1        157474.10  \n",
       "9974          1               0         13549.24  \n",
       "9975          1               0        196526.55  \n",
       "9976          1               0         84419.78  \n",
       "9977          1               1         24991.92  \n",
       "9978          1               0        191608.97  \n",
       "9979          0               1          2988.28  \n",
       "9980          0               0         99595.67  \n",
       "9981          1               1         53445.17  \n",
       "9982          1               0        115146.40  \n",
       "9983          0               0        151325.24  \n",
       "9984          1               1         51695.41  \n",
       "9985          1               0         96833.00  \n",
       "9986          0               1         34047.54  \n",
       "9987          1               1          1914.41  \n",
       "9988          1               0         49337.84  \n",
       "9989          1               1        179436.60  \n",
       "9990          1               0         53667.08  \n",
       "9991          1               0         69384.71  \n",
       "9992          1               0        195192.40  \n",
       "9993          1               0         29179.52  \n",
       "9994          0               0        167773.55  \n",
       "9995          1               0         96270.64  \n",
       "9996          1               1        101699.77  \n",
       "9997          0               1         42085.58  \n",
       "9998          1               0         92888.52  \n",
       "9999          1               0         38190.78  \n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y output values:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "5       1\n",
       "6       0\n",
       "7       1\n",
       "8       0\n",
       "9       0\n",
       "10      0\n",
       "11      0\n",
       "12      0\n",
       "13      0\n",
       "14      0\n",
       "15      0\n",
       "16      1\n",
       "17      0\n",
       "18      0\n",
       "19      0\n",
       "20      0\n",
       "21      0\n",
       "22      1\n",
       "23      0\n",
       "24      0\n",
       "25      0\n",
       "26      0\n",
       "27      0\n",
       "28      0\n",
       "29      0\n",
       "       ..\n",
       "9970    0\n",
       "9971    0\n",
       "9972    0\n",
       "9973    0\n",
       "9974    0\n",
       "9975    1\n",
       "9976    0\n",
       "9977    0\n",
       "9978    0\n",
       "9979    0\n",
       "9980    0\n",
       "9981    1\n",
       "9982    1\n",
       "9983    0\n",
       "9984    0\n",
       "9985    0\n",
       "9986    0\n",
       "9987    0\n",
       "9988    0\n",
       "9989    0\n",
       "9990    0\n",
       "9991    1\n",
       "9992    0\n",
       "9993    0\n",
       "9994    0\n",
       "9995    0\n",
       "9996    0\n",
       "9997    1\n",
       "9998    1\n",
       "9999    0\n",
       "Name: Exited, Length: 10000, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Artificial Neural Network\n",
    "# Part 1 - Import Data and Extract input 'x' and output 'y'\n",
    "\n",
    "# Pip or Conda Install these libraries in the Terminal\n",
    "# Install Theano (U. Montreal, GPU or CPU parallel Float Point computation)\n",
    "# Install Tensorflow (Google, same as above)\n",
    "# Install Keras (Combines the above 2 libraries with a high-level API)\n",
    "\n",
    "# Numpy is a high speed Math computation library\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib is used for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pandas is a database analysis tool\n",
    "import pandas as pd\n",
    "\n",
    "# LabelEncoder is used to encode binary categorical data into numbers (male/female -> 0/1)\n",
    "# OneHotEncoder is used to encode categorical data with more than 2 possible options (France, Spain, Germany)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Split the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature Scaling eases computation by standardizing input data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Your CSV data URL link goes here\n",
    "data_url = 'https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Data/Bank_Customer_Data.csv'\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv(data_url)\n",
    "\n",
    "# Display data to make sure it has been imported\n",
    "display(dataset)\n",
    "\n",
    "# Extract input Independent Variable (Matrix of Features and Observations)\n",
    "# Row number, customer ID, and name are not useful, so they're excluded\n",
    "x = dataset.iloc[:, 3:13]\n",
    "\n",
    "# Extract output Dependant Variables. The last column shows whether a customer left of stayed with the bank\n",
    "y = dataset.iloc[:, 13]\n",
    "\n",
    "print('X input values:')\n",
    "display(x)\n",
    "\n",
    "print('Y output values:')\n",
    "display(y)\n",
    "\n",
    "# Use the '.values' method to convert data from Pandas Dataframes to NumPy arrays\n",
    "x = x.values\n",
    "y = y.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network\n",
    "# Part 2 - Data pre-processing\n",
    "\n",
    "# In the Bank example: Convert France/Germany/Spain into 0/1/2\n",
    "labelencoder_x_1 = LabelEncoder()\n",
    "x[:, 1] = labelencoder_x_1.fit_transform(x[:, 1])\n",
    "\n",
    "# In the Bank example: Convert Female/Male into 0/1\n",
    "labelencoder_x_2 = LabelEncoder()\n",
    "x[:, 2] = labelencoder_x_2.fit_transform(x[:, 2])\n",
    "\n",
    "# Country data is categorical but not ordinal (order doesn't matter), create dummy variables with OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "\n",
    "# Make all Dependent X objects have the same type (Float 64)\n",
    "x = onehotencoder.fit_transform(x).toarray()\n",
    "\n",
    "# Remove first column to avoid Dummy Variable trap\n",
    "# Two columns of binary data is enough to describe 3 categories (France, Spain, Germany)\n",
    "x = x[:, 1:]\n",
    "\n",
    "\n",
    "# Create Training and Test sets and apply Feature Scaling (standardize)\n",
    "\n",
    "# Encode the Dependent Variable\n",
    "# In Bank example we don't need to encode Dependent variables because it's already Binary\n",
    "# If there are more outputs to examine, uncomment the following lines of code\n",
    "#labelencoder_y = LabelEncoder()\n",
    "#y = labelencoder_y.fit_transform(y)\n",
    "\n",
    "# Test_size = 0.2 means 80% of data for training, 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 0.8061 - acc: 0.7371\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 0.6665 - acc: 0.7960\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 0.7100 - acc: 0.7970\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.6955 - acc: 0.8219\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 0.7069 - acc: 0.8232\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 0.7276 - acc: 0.8255\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.7112 - acc: 0.8250\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 0.7512 - acc: 0.8271\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.7065 - acc: 0.8281\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.6796 - acc: 0.8286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a33800be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Artificial Neural Network\n",
    "# Part 3 - Artificial Neural Network Architecture\n",
    "\n",
    "# Importing the Keras library\n",
    "import keras\n",
    "\n",
    "# Sequential is used to initialize NN\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Dense is used to build Deep layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Dropout  is used to prevent overfitting, by using Dropout Regularization\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialising the ANN Sequentially (can also initialize as Graph)\n",
    "# We use Sequential Classifier because we have successive layers\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "# This step initializes the Wights to small random numbers\n",
    "# 'Units' is the number of hidden layers (begin with average of Input & Output layers = 11+1/2 = 6)\n",
    "# 'Kernel_initializer': Initialize weights as small random numbers\n",
    "# 'Input_dim': number Independent Variables\n",
    "# 'Activation': Rectifier Activation Function ('relu') for Hidden Layers, Sigmoid Function for Output Layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "# Add Dropout Regularization to first layer to prevent overfitting\n",
    "# 'p': Fraction of Neurons to drop. Start with 0.1 (10% dropped) and increment by 0.1 until overfitting is solved, don't go over 0.5\n",
    "classifier.add(Dropout(rate = 0.1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dropout(rate = 0.1))\n",
    "\n",
    "# Add the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "classifier.add(Dropout(rate = 0.1))\n",
    "\n",
    "# Compile the ANN\n",
    "# 'optimizer': Algorithm used to find the best Weights. 'adam' is a popular Stochastic Gradient Descent Algorithm\n",
    "# 'loss' = 'binary_crossentropy' is useful for Binary Outputs with logarithmic functions\n",
    "# 'loss' = 'categorical_crossentropy' is useful for 3+ categorical Outputs\n",
    "# 'metrics' =  Used to evaluate the ANN, requires list. We use 1 metric called 'accuracy'  \n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "# Experiment to find best 'batch_size' and 'epochs'\n",
    "classifier.fit(x_train, y_train, batch_size = 10, epochs = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model was trained with an accuracy of 83.0 %\n",
      "\n",
      "For a new sample input:\n",
      " [0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000] \n",
      "\n",
      "Prediction - Will this customer leave the Bank?\n",
      " Result =  [[False]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Artificial Neural Network\n",
    "# Part 4 - Making predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "# This gives a vector of probablities of Customers leaving the bank\n",
    "# You can rank the probabilities of customers most likely to leave the bank\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "\n",
    "# Choose a threshold of which customers leave or stay (use 50% as a starting threshold)\n",
    "# This line converts probabilities into True/False\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "\n",
    "# Predicting a single new observation\n",
    "# Predict if the customer with the following informations will leave the bank:\n",
    "# Geography: France\n",
    "# Credit Score: 600\n",
    "# Gender: Male\n",
    "# Age: 40\n",
    "# Tenure: 3\n",
    "# Balance: 60000\n",
    "# Number of Products: 2\n",
    "# Has Credit Card: Yes\n",
    "# Is Active Member: Yes\n",
    "# Estimated Salary: 50000\n",
    "# sc.transform Feature Scales the new prediction so the model will understand it\n",
    "# Set 1 element as a float64 to set all to float64\n",
    "\n",
    "# Change this sample input to test a new prediction\n",
    "sample_input = [0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]\n",
    "\n",
    "new_prediction = classifier.predict(sc.transform(np.array([sample_input])))\n",
    "new_prediction = (new_prediction > 0.5)\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "# Tells you the number of correct vs. incorrect observations\n",
    "# In the Confusion Matrix we get [1,1] + [2,2] Correct Predictions\n",
    "# In the Confusion Matrix we get [1,2] + [2,1] Incorrect Predictions\n",
    "# Compute accuracy = correct predictions / total predictions\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Measure accuracy percentage of the Training Set\n",
    "accuracy = (cm[0,0] + cm[1,1])/2000*100\n",
    "\n",
    "print('This model was trained with an accuracy of', accuracy, '%\\n')\n",
    "\n",
    "print('For a new sample input:\\n', sample_input, '\\n')\n",
    "\n",
    "print('Prediction - Will this customer leave the Bank?\\n', 'Result = ', new_prediction, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 1s 132us/step - loss: 0.6230 - acc: 0.7968\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 0s 59us/step - loss: 0.4542 - acc: 0.7970\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 0s 57us/step - loss: 0.4363 - acc: 0.7970\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 0s 59us/step - loss: 0.4330 - acc: 0.7970\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 0s 57us/step - loss: 0.4304 - acc: 0.7970\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 1s 158us/step - loss: 0.6270 - acc: 0.7945\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 0.4532 - acc: 0.7950\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 0s 71us/step - loss: 0.4343 - acc: 0.7950\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 0s 59us/step - loss: 0.4295 - acc: 0.7950\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 0s 68us/step - loss: 0.4267 - acc: 0.7950\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 1s 131us/step - loss: 0.6372 - acc: 0.7965\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 0s 68us/step - loss: 0.4875 - acc: 0.7970\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 0.4426 - acc: 0.7970\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 0.4359 - acc: 0.7970\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 0s 57us/step - loss: 0.4321 - acc: 0.7970\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 1s 147us/step - loss: 0.6363 - acc: 0.7938\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 0s 54us/step - loss: 0.4887 - acc: 0.7950\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 0s 73us/step - loss: 0.4406 - acc: 0.7950\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 0s 72us/step - loss: 0.4334 - acc: 0.7950\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 0s 70us/step - loss: 0.4292 - acc: 0.7950\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 1s 196us/step - loss: 0.6448 - acc: 0.7942\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 0s 66us/step - loss: 0.4726 - acc: 0.7970\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 0s 71us/step - loss: 0.4406 - acc: 0.7970\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 0s 69us/step - loss: 0.4347 - acc: 0.7970\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 0.4317 - acc: 0.7970\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 0s 63us/step - loss: 0.4288 - acc: 0.7970\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 0s 66us/step - loss: 0.4267 - acc: 0.7970\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 0.4241 - acc: 0.7970\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 0s 60us/step - loss: 0.4217 - acc: 0.8090\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 0s 73us/step - loss: 0.4193 - acc: 0.8222\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 1s 188us/step - loss: 0.6253 - acc: 0.7950\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 0s 66us/step - loss: 0.4562 - acc: 0.7950\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 0.4342 - acc: 0.7950\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 0s 67us/step - loss: 0.4299 - acc: 0.7950\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 0s 74us/step - loss: 0.4278 - acc: 0.7950\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 0s 61us/step - loss: 0.4265 - acc: 0.7950\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 0s 67us/step - loss: 0.4251 - acc: 0.7950\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 0s 68us/step - loss: 0.4240 - acc: 0.7950\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 0s 61us/step - loss: 0.4231 - acc: 0.7950\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 0.4221 - acc: 0.7950\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 1s 175us/step - loss: 0.6325 - acc: 0.7940\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 0.4844 - acc: 0.7970\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 0s 63us/step - loss: 0.4426 - acc: 0.7970\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 0s 60us/step - loss: 0.4363 - acc: 0.7970\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 0s 69us/step - loss: 0.4325 - acc: 0.7970\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 0s 66us/step - loss: 0.4300 - acc: 0.7970\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 0s 63us/step - loss: 0.4278 - acc: 0.7970\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 0s 69us/step - loss: 0.4257 - acc: 0.7970\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 0s 56us/step - loss: 0.4242 - acc: 0.7970\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 0s 54us/step - loss: 0.4226 - acc: 0.7970\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 1s 169us/step - loss: 0.6391 - acc: 0.7920\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 0s 71us/step - loss: 0.4906 - acc: 0.7950\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 0.4430 - acc: 0.7950\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 0.4346 - acc: 0.7950\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 0s 72us/step - loss: 0.4302 - acc: 0.7950\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 0s 65us/step - loss: 0.4277 - acc: 0.7950\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 0s 70us/step - loss: 0.4261 - acc: 0.7950\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 0s 65us/step - loss: 0.4247 - acc: 0.7950\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 0s 60us/step - loss: 0.4230 - acc: 0.7950\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 0s 65us/step - loss: 0.4215 - acc: 0.7950\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 1s 197us/step - loss: 0.6612 - acc: 0.7963\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 0s 61us/step - loss: 0.5328 - acc: 0.7970\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 0s 58us/step - loss: 0.4506 - acc: 0.7970\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 0s 61us/step - loss: 0.4335 - acc: 0.7970\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 0s 55us/step - loss: 0.4270 - acc: 0.7970\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 1s 204us/step - loss: 0.6645 - acc: 0.7913\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 0s 71us/step - loss: 0.5242 - acc: 0.7950\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 0s 73us/step - loss: 0.4449 - acc: 0.7950\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 0s 63us/step - loss: 0.4333 - acc: 0.7950\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 0s 60us/step - loss: 0.4282 - acc: 0.7950\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 1s 189us/step - loss: 0.6560 - acc: 0.7960\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 0s 54us/step - loss: 0.5486 - acc: 0.7970\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 0s 62us/step - loss: 0.4626 - acc: 0.7970\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 0s 68us/step - loss: 0.4403 - acc: 0.7970\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 0s 57us/step - loss: 0.4323 - acc: 0.7970\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 1s 198us/step - loss: 0.6602 - acc: 0.7940\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 0s 60us/step - loss: 0.5671 - acc: 0.7950\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 0s 62us/step - loss: 0.4719 - acc: 0.7950\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 0s 56us/step - loss: 0.4407 - acc: 0.7950\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 0s 57us/step - loss: 0.4346 - acc: 0.7950\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 1s 224us/step - loss: 0.6555 - acc: 0.7925\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.4904 - acc: 0.7970\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 0s 65us/step - loss: 0.4442 - acc: 0.7970\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 0s 74us/step - loss: 0.4375 - acc: 0.7970\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 0s 64us/step - loss: 0.4338 - acc: 0.7970\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 0s 63us/step - loss: 0.4316 - acc: 0.7970\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 0s 55us/step - loss: 0.4297 - acc: 0.7970\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 0s 53us/step - loss: 0.4280 - acc: 0.7970\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 0s 57us/step - loss: 0.4265 - acc: 0.7970\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 0s 71us/step - loss: 0.4252 - acc: 0.7970\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 1s 247us/step - loss: 0.6572 - acc: 0.7943\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 0s 60us/step - loss: 0.4952 - acc: 0.7950\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 0s 68us/step - loss: 0.4362 - acc: 0.7950\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 0s 63us/step - loss: 0.4306 - acc: 0.7950\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 0s 66us/step - loss: 0.4283 - acc: 0.7950\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 0s 61us/step - loss: 0.4266 - acc: 0.7950\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 0s 62us/step - loss: 0.4253 - acc: 0.7950\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 0s 64us/step - loss: 0.4241 - acc: 0.7950\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 0s 62us/step - loss: 0.4227 - acc: 0.7950\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 0s 61us/step - loss: 0.4214 - acc: 0.7950\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 1s 232us/step - loss: 0.6685 - acc: 0.7933\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.5910 - acc: 0.7970\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 0.4894 - acc: 0.7970\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 0s 54us/step - loss: 0.4469 - acc: 0.7970\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 0s 54us/step - loss: 0.4342 - acc: 0.7970\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 0.4282 - acc: 0.7970\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 0s 56us/step - loss: 0.4240 - acc: 0.7970\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 0s 54us/step - loss: 0.4202 - acc: 0.7970\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 0s 61us/step - loss: 0.4165 - acc: 0.7965\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 0.4129 - acc: 0.8127\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 1s 242us/step - loss: 0.6501 - acc: 0.7947\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 0s 56us/step - loss: 0.5223 - acc: 0.7950\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 0s 61us/step - loss: 0.4496 - acc: 0.7950\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 0s 60us/step - loss: 0.4367 - acc: 0.7950\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 0s 59us/step - loss: 0.4321 - acc: 0.7950\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 0s 74us/step - loss: 0.4289 - acc: 0.7950\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 0s 73us/step - loss: 0.4270 - acc: 0.7950\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.4256 - acc: 0.7950\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 0s 68us/step - loss: 0.4244 - acc: 0.7950: 0s - loss: 0.4246 - acc: 0.\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 0s 68us/step - loss: 0.4230 - acc: 0.7950\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 147us/step - loss: 0.5834 - acc: 0.7960\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 0.4456 - acc: 0.7960\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 0.4331 - acc: 0.7960\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 0.4279 - acc: 0.7960\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 0.4242 - acc: 0.7960\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 0.4207 - acc: 0.8073\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.4178 - acc: 0.8246\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 0.4147 - acc: 0.8285\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 0.4126 - acc: 0.8313\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 0.4104 - acc: 0.8325\n",
      "\n",
      "\n",
      "\n",
      "This model was trained with an accuracy of 81.08%\n",
      "\n",
      "\n",
      "The best hyper-parameters for this model:\n",
      " {'batch_size': 32, 'epochs': 10, 'optimizer': 'rmsprop'} \n",
      "\n",
      "\n",
      "Use the above Hyper-Parameters to retrain your model and make improved predictions.\n"
     ]
    }
   ],
   "source": [
    "# Artificial Neural Network\n",
    "# Part 5 - Improve and Tune Hyper-Parameters for the ANN (takes a long time to compute)\n",
    "\n",
    "# This method is the most robust architecture, allows you to find the best hyper-parameters and accuracies\n",
    "\n",
    "# Dropout Regularization to reduce overfitting if needed\n",
    "# GridSearch tries several Tuning Hyper Parameters to find the best ones\n",
    "\n",
    "# K-Fold Cross Validation breaks up the data into 'K' chunks\n",
    "# It then trains 'K' times, choosing a different chunk every time, this improves accuracy\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Import data and extract input x and output y\n",
    "data_url = 'https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Database-Prediction/master/Data/Bank_Customer_Data.csv'\n",
    "dataset = pd.read_csv(data_url)\n",
    "x = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# Data pre-processing\n",
    "labelencoder_x_1 = LabelEncoder()\n",
    "x[:, 1] = labelencoder_x_1.fit_transform(x[:, 1])\n",
    "\n",
    "labelencoder_x_2 = LabelEncoder()\n",
    "x[:, 2] = labelencoder_x_2.fit_transform(x[:, 2])\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "x = onehotencoder.fit_transform(x).toarray()\n",
    "x = x[:, 1:]\n",
    "\n",
    "# Prepare training and test sets, also apply feature scaling\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "\n",
    "# This function has an input (Optimizer) so we can try different ones\n",
    "# 'Adam' and 'rmsprop' (also good for RNN) are good optimizers for stochastic gradient descent\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# Build Neural Network Classifier with K-Fold Cross Validation training, tune Hyper-Parameters here\n",
    "# Try 'epochs': [100, 500] for major improvements to accuracy\n",
    "# Try 'cv = 10' for increased K-Validation segmentation\n",
    "classifier = KerasClassifier(build_fn = build_classifier)\n",
    "parameters = {'batch_size': [25, 32],\n",
    "              'epochs': [5, 10],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2)\n",
    "\n",
    "\n",
    "# Fit Model to data using grid_search to try various Hyper Parameter\n",
    "grid_search = grid_search.fit(x_train, y_train)\n",
    "# Output best parameters\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "\n",
    "print('\\n\\n\\nThis model was trained with an accuracy of %.2f%%\\n' % (best_accuracy*100))\n",
    "\n",
    "print('\\nThe best hyper-parameters for this model:\\n', best_parameters, '\\n')\n",
    "\n",
    "print('\\nUse the above Hyper-Parameters to retrain your model and make improved predictions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "439.969px",
    "left": "997.469px",
    "right": "20px",
    "top": "35.9688px",
    "width": "457.969px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
